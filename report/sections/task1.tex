% task1.tex

% Section Title
\section{TASK 1 --- DATA ANALYSIS AND PREPROCESSING} \label{sec:data-analysis-preprocessing}

    \subsection{Dataset Overview}

        The raw dataset contained \textbf{31,507 samples} and \textbf{17 features}, including numerical flow statistics and a categorical label identifying traffic types: \textit{Benign} ($\approx$ 63\% of the samples), \textit{DoS Hulk}, \textit{PortScan}, and \textit{Brute Force}.
        A class distribution plot (Figure \ref{fig:class_distribution}) visually confirmed this imbalance, motivating the use of \textbf{stratified splitting} later in the pipeline.

        \begin{figure}[H]
            \centering
            \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\linewidth]{images/task1/class_distribution.png}
                \vspace{-0.5cm}
                \caption{Class distribution in the raw dataset.}
                \label{fig:class_distribution}
            \end{minipage}
            \hfill
            \begin{minipage}{0.48\textwidth}
                \centering
                \begin{tabular}{l l l}
                    \toprule
                    \textbf{Step} & \textbf{Removed} & \textbf{Remaining} \\
                    \midrule
                    Drop NaN             & 20    & 31,487 \\
                    Drop duplicates      & 2,114 & 29,393 \\
                    Drop infinite values & 7     & \textbf{29,386} \\
                    \bottomrule
                \end{tabular}
                \vspace{0.3cm}
                \captionof{table}{Summary of the raw dataset.}
                \label{tab:cleaning_steps}
            \end{minipage}
        \end{figure}

        Before any training, the dataset must be cleaned and normalized to ensure the model learns meaningful statistical relationships rather than artifacts of data noise or imbalance.
        The preprocessing pipeline addresses three critical aspects: \textbf{data quality} through the removal of missing, duplicate, and infinite values; \textbf{class imbalance} by preserving proportional representation during splitting; and \textbf{feature scaling} to standardize feature ranges and stabilize neural training convergence.

    \subsection{Data Cleaning and Partitioning}

        Table \ref{tab:cleaning_steps} summarizes the cleaning process: duplicate rows, missing values (NaN), and infinite entries were systematically removed.
        Thus, the dataset was reduced to \textbf{29,386 samples}, meaning \textbf{2,121 rows} were discarded (2,114 between missing and duplicates, and 7 infinite values).
        Categorical labels were then encoded numerically: \textit{Benign} = 0, \textit{Brute Force} = 1, \textit{DoS Hulk} = 2, \textit{PortScan} = 3.

        The cleaned data were divided into training (60\%, 17,631 samples), validation (20\%, 5,877 samples), and test (20\%, 5,878 samples) subsets using \textbf{stratified sampling} with a fixed seed, ensuring consistent class proportions and reproducibility.
        Exploration of numerical attributes revealed high variability and heavy-tailed distributions (Figures~\ref{fig:raw_distributions_1}), indicating the presence of outliers and the need for normalization.

        \vspace{-0.2cm}

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\linewidth]{images/task1/raw_distributions_1.png}
            \vspace{-0.5cm}
            \caption{Examples of feature (Bwd Packet Length Max) kernel density plot and boxplot, highlighting outliers.}
            \label{fig:raw_distributions_1}
        \end{figure}

        \vspace{-0.3cm}

    \subsection{Outlier Detection and Normalization}

        Outliers were analyzed through the \textbf{Z-score} and \textbf{IQR} methods. 
        Both confirmed extreme values in features such as \textit{Bwd Packet Length Max}, \textit{Flow Duration}, and \textit{Fwd IAT Std}. 
        Outliers were retained to preserve data realism, and normalization was applied to reduce their effect.

        Two scaling methods were tested: \textbf{StandardScaler} and \textbf{RobustScaler}.
        The density comparison plots (Figure \ref{fig:comparison_normalization}) showed that both methods effectively normalized distributions, but RobustScaler produced more compact, less skewed curves for highly variable features.
        However, the numerical statistics and subsequent training experiments revealed negligible performance difference between the two.
        Therefore, \textbf{StandardScaler} was ultimately adopted for simplicity and interpretability, offering smoother loss curves in preliminary trials.

        \begin{figure}[H]
            \centering
            \begin{subfigure}[b]{0.48\linewidth}
                \centering
                \includegraphics[width=0.9\linewidth]{images/task1/raw_distributions_comparison_1.png}
                \label{fig:raw_distributions_comparison_1}
            \end{subfigure}
            \hfill
            \begin{subfigure}[b]{0.48\linewidth}
                \centering
                \includegraphics[width=0.9\linewidth]{images/task1/raw_distributions_comparison_2.png}
                \label{fig:raw_distributions_comparison_2}
            \end{subfigure}
            \vspace{-0.2cm}
            \caption{Comparison of normalization effects using StandardScaler and RobustScaler.}
            \label{fig:comparison_normalization}
        \end{figure}

        \noindent This preprocessing ensured consistent, balanced, and properly scaled data, forming a robust foundation for training the Feed-Forward Neural Network.

\clearpage % Remove if not needed
