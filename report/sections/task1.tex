\section{DATA ANALYSIS AND PREPROCESSING}
\label{sec:data-analysis-preprocessing}

\subsection{Raw Dataset Profile}
The dataset, derived from CICIDS2017, contains labeled network flows with statistical, temporal, and content-based features. It initially included \textbf{31,507 samples}, combining benign and attack traffic. The class distribution (Figure~\ref{fig:class_distribution}) shows a strong imbalance, with benign samples dominating.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{images/task1/class_distribution.png}
    \caption{Class distribution in the raw dataset.}
    \label{fig:class_distribution}
\end{figure}

Exploration of numerical attributes revealed high variability and heavy-tailed distributions (Figures~\ref{fig:raw_distributions_1}), indicating the presence of outliers and the need for normalization.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{images/task1/raw_distributions_1.png}
    \caption{Examples of feature (Bwd Packet Length Max) kernel density plot and boxplot, highlighting outliers.}
    \label{fig:raw_distributions_1}
\end{figure}

\subsection{Data Cleaning and Partitioning}
After removing missing, duplicate, and infinite entries, the dataset was reduced to \textbf{29,386 samples}, meaning \textbf{2,121 rows} were discarded (2,114 missing/duplicates and 7 infinite values).

\noindent The cleaned data were divided into training (60\%), validation (20\%), and test (20\%) subsets using stratified sampling with a fixed seed, ensuring consistent class proportions and reproducibility.

\subsection{Outlier Detection and Normalization}
Outliers were analyzed through the \textit{Z-score} and \textit{IQR} methods. Both confirmed extreme values in features such as \textit{Bwd Packet Length Max}, \textit{Flow Duration}, and \textit{Fwd IAT Std}. Outliers were retained to preserve data realism, and normalization was applied to reduce their effect.

Two scaling methods were tested: \textit{StandardScaler} and \textit{RobustScaler}. The latter, based on median and IQR, produced tighter and less skewed distributions, while StandardScaler—though sensitive to outliers—yielded smoother training dynamics. Hence, \textbf{StandardScaler} was chosen for the final preprocessing stage.

\begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/task1/raw_distributions_comparison_1.png}
        \caption{Bwd Packet Length Max.}
        \label{fig:raw_distributions_comparison_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/task1/raw_distributions_comparison_2.png}
        \caption{Fwd IAT Std.}
        \label{fig:raw_distributions_comparison_2}
    \end{subfigure}
    \caption{Comparison of normalization effects using StandardScaler and RobustScaler.}
    \label{fig:comparison_normalization}
\end{figure}

\noindent This preprocessing ensured consistent, balanced, and properly scaled data, forming a robust foundation for training the Feed-Forward Neural Network.
