% task4.tex

% Section Title
\section{TASK 4 --- THE IMPACT OF THE LOSS FUNCTION (CLASS WEIGHTED)} \label{sec:impact-loss-function}

    \subsection{Removing the Destination Port Feature}

        The best-performing model from previous tasks (64 neurons, ReLU activation) was retrained after excluding the \textit{Destination Port} feature to eliminate the bias discussed earlier. 
        This modification had a mixed effect: overall accuracy remained stable, and performance on the \textit{Brute Force} class slightly improved, confirming that the model no longer relied on a biased feature. 
        However, the ability to recognize the rarest class, \textit{PortScan}, declined significantly (F1-score dropped from 0.92 to 0.38), suggesting that the model had previously exploited this feature as a strong shortcut for \textit{PortScan} detection.

        \begin{figure}[H]
            \centering
            \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\linewidth]{images/task4/loss_curves_model_relu_64_no_port.png}
                \caption{Loss curves for the best model (no Destination Port).}
                \label{fig:loss_no_port}
            \end{minipage}
            \hfill
            \begin{minipage}{0.48\textwidth}
                \centering
                \begin{tabular}{lcccc}
                    \toprule
                    \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
                    \midrule
                    \textbf{0} \textit{Benign}      & 0.9612 & 0.9748 & 0.9680 \\
                    \textbf{1} \textit{Brute Force} & 0.8018 & 0.9476 & 0.8686 \\
                    \textbf{2} \textit{DoS Hulk}    & 0.9857 & 0.8900 & 0.9354 \\
                    \textbf{3} \textit{Port Scan}   & 0.5312 & 0.2982 & 0.3820 \\
                    \midrule
                \end{tabular}
                \begin{tabular}{c c c}
                    \textbf{Accuracy} &  \textbf{Macro F1} & \textbf{Weighted F1} \\
                    0.9499 & 0.7885 & 0.9486 \\
                \end{tabular}
                \vspace{0.3cm}
                \caption{Test set report.}
                \label{tab:linear-metrics}
            \end{minipage}
        \end{figure}

    \subsection{Class Weights and Weighted Loss}

        To counter the imbalance identified in previous tasks, the strategy was to apply class weights in the loss function.
        Weighted cross-entropy was then adopted to penalize misclassifications of minority classes more strongly, encouraging the model to learn balanced decision boundaries.
        
        \vspace{0.5cm}

        \begin{center}
            \begin{tabular}{c c c c}
                \toprule
                \textbf{Benign} & \textbf{Brute Force} & \textbf{DoS Hulk} & \textbf{Port Scan}  \\
                \midrule
                0.3326014 & 3.93720794 & 1.45206807 & 19.70906433 \\
                \bottomrule
            \end{tabular}
            \vspace{0.1cm}
            \captionof{table}{Class weights used for weighted cross-entropy loss.}
            \label{tab:class-weights}
        \end{center}

        \vspace{0.1cm}

        \noindent
        These weights were estimated on the training partition to avoid data leakage. 
        Estimating class weights from training data ensures that information from validation or test sets is not used during training or weight calculation, and allows the weighted loss to emphasize minority classes while preserving evaluation integrity.

    \subsection{Effect of Weighted Cross-Entropy}
    
        Training with the weighted loss produced smoother convergence and more balanced class performance (Figure~\ref{fig:loss_weighted}). 
        Overall accuracy and weighted F1 decreased slightly (accuracy 0.9499 $\rightarrow$ 0.9221; weighted F1 0.9486 $\rightarrow$ 0.9325), while recall for underrepresented classes improved markedly â€” \textit{PortScan} recall rose from 0.2982 to 0.8421 and \textit{Brute Force} recall increased slightly from 0.9476 to 0.9545. 
        This confirms that the weighted cross-entropy promotes fairness across classes by reducing bias toward dominant traffic types, at the cost of a small drop in global accuracy.

        \begin{figure}[H]
            \centering
            \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\linewidth]{images/task4/loss_curves_model_relu_64_no_port_weighted.png}
                \caption{Loss curves (weighted cross-entropy).}
                \label{fig:loss_weighted}
            \end{minipage}
            \hfill
            \begin{minipage}{0.48\textwidth}
                \centering
                \begin{tabular}{lcccc}
                    \toprule
                    \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
                    \midrule
                    \textbf{0} \textit{Benign}      & 0.9790 & 0.9254 & 0.9515 \\
                    \textbf{1} \textit{Brute Force} & 0.7358 & 0.9545 & 0.8311 \\
                    \textbf{2} \textit{DoS Hulk}    & 0.9574 & 0.9017 & 0.9287 \\
                    \textbf{3} \textit{Port Scan}   & 0.2376 & 0.8421 & 0.3707 \\
                    \midrule
                \end{tabular}
                \begin{tabular}{c c c}
                    \textbf{Accuracy} &  \textbf{Macro F1} & \textbf{Weighted F1} \\
                    0.9221 & 0.7705 & 0.9325 \\
                \end{tabular}
                \vspace{0.3cm}
                \caption{Test set report.}
                \label{tab:linear-metrics}
            \end{minipage}
        \end{figure}

        \noindent In summary, applying a class-weighted loss increased sensitivity to underrepresented attacks, but this came at the expense of precision (\textit{PortScan} precision 0.5312 $\rightarrow$ 0.2376) and a small drop in overall metrics.
        This experiment highlights the trade-off between improving per-class recall for minority classes and reducing precision and global performance.
