% task3.tex

% Section Title
% \section{THE IMPACT OF SPECIFIC FEATURES (DESTINATION PORT)} \label{sec:impact-dest-port}
\section{TASK 3 --- IMPACT OF SPECIFIC FEATURES (DESTINATION PORT)} \label{sec:impact-dest-port}

    % \subsection{Why this feature matters}
    % 
    %     In the provided dataset, \emph{Destination Port} is strongly correlated with certain labels. 
    %     In particular, all flows labeled as \emph{Brute Force} originate from port 80. 
    %     If a model exploits this shortcut, it will appear accurate under the original distribution but fail under even small deviations at inference time. 
    %     We therefore design two experiments to diagnose and mitigate this bias.
    % 
    % \subsection{Perturbation at inference time}
    % 
    %     We modify the \emph{test} split only by replacing \texttt{Destination Port} = 80 with 8080 for rows labeled \emph{Brute Force}, leaving all other samples unchanged. 
    %     Evaluating the best shallow model on this perturbed test set reveals a pronounced drop in precision (\textasciitilde\,0.17), recall (\textasciitilde\,0.05), and F1 (\textasciitilde\,0.08) for \emph{Brute Force}, with overall accuracy falling from \textasciitilde\,95\% to \textasciitilde\,90.4\%. 
    %     This confirms that the original model captured a spurious correlation rather than a causal signal.
    % 
    % \subsection{Removing the feature and reprocessing}
    % 
    %     To remove the shortcut entirely, we drop the \texttt{Destination Port} column from the original table and repeat the full preprocessing (cleaning, stratified splitting, scaling fitted on training). 
    %     An immediate side effect is a sharp change in class supports: without the port column, many rows that previously differed only by destination port become exact duplicates and are eliminated by the cleaning step. 
    %     The net effect is that some attack classes --- most notably \emph{PortScan} --- lose a large fraction of instances. 
    %     For example, \emph{PortScan} counts drop from \textasciitilde\,5,000 in the raw dataset to \textasciitilde\,285 after duplicate removal on the no-port table; \emph{Brute Force} support in validation/test also becomes very small (\textasciitilde\,57 samples per split in our runs).
    % 
    %     This phenomenon illustrates that ``debiasing by feature removal'' can reshape the dataset in subtle ways. 
    %     It is still the right choice to prevent shortcut learning here, but it must be accompanied by evaluation strategies that remain meaningful when some classes become rare.
    % 
    % \subsection{Implications}
    % 
    %     After removing the feature, training on the new splits yields models that no longer rely on ports, but class imbalance is now more severe. 
    %     Consequently, downstream experiments (next sections) complement this change with class-weighted losses and careful reporting of macro metrics to avoid over-optimistic conclusions. 
    %     Where class supports become extremely small, we emphasize macro-F1 and per-class recall, and we discuss the limitations of single-split estimates.

    \subsection{Objective}

        The third task investigates \textbf{feature-induced bias} within the dataset—-specifically the influence of the \textbf{“Destination Port”} attribute on the trained FFNN's decisions.
        In intrusion-detection datasets, port numbers can act as strong but misleading indicators of attack types. 
        Here, all \textit{Brute Force} attacks in the CICIDS2017 subset used \textbf{port 80}, creating an artificial correlation between this port and the attack label.
        The objectives were therefore to:

        \begin{enumerate}
            \item Evaluate how dependent the trained model is on this biased feature;
            \item Examine the degradation of classification performance when the port information is modified;
            \item Quantify how dataset cleaning changes class balance when the port feature is removed;
            \item Discuss whether the Destination Port feature should be retained in future models.
        \end{enumerate}

    % \subsection{Findings}
    % 
    %     When \textit{Brute Force} samples were changed from port 80 to 8080, detection collapsed (F1 $\rightarrow 0.08$), revealing strong over-reliance on this feature.  
    %     Removing \texttt{Destination Port} exposed redundant \textit{PortScan} samples, confirming artificial correlations.
    % 
    % \subsection{Discussion}
    % 
    %     The model had learned port-based shortcuts rather than true attack behavior.  
    %     Feature removal improved realism and fairness, ensuring generalizable learning.

    \subsection{Methodology}

        Two complementary experiments were performed using the \textbf{best-performing ReLU FFNN (64 neurons)} obtained in Task 2.

        \textbf{Experiment 1 —- Feature Perturbation}

            \begin{itemize}
                \item The test set was modified such that all instances of the \textit{Brute Force} class (label = 1) originally using \textbf{Destination Port = 80} were reassigned to \textbf{Port 8080}.
                \item No other features were changed; scaling was reapplied using the same \texttt{StandardScaler} fitted on the training data.
                \item The modified test set was then used for inference with the trained model, and the results were compared against the original validation report.
            \end{itemize}

        \textbf{Experiment 2 —- Feature Removal}

            \begin{itemize}
                \item The \textit{Destination Port} feature was completely removed from the raw dataset.
                \item The full preprocessing pipeline (NaN removal, duplicate cleaning, scaling, encoding) was repeated.
                \item Class distributions—-especially for \textit{PortScan} traffic—-were compared before and after cleaning to reveal the extent to which this feature had contributed to duplicated or redundant samples.
            \end{itemize}    

    \subsection{Results —- Experiment 1: Port Perturbation}

        When the port numbers of \textit{Brute Force} samples were changed from \(80 \rightarrow 8080\), model performance dropped sharply:

        \begin{table}
            \centering
            \begin{tabular}{lcc}
                \toprule
                \textbf{Metric} & \textbf{Validation (Baseline)} & \textbf{Modified Test (Port 8080)} \\
                \midrule
                Accuracy                & 0.954 & 0.908 \\
                Macro F1                & 0.928 & 0.723 \\
                \textit{Brute Force} F1 & 0.85  & 0.08  \\
                \textit{Benign} F1      & 0.97  & 0.93  \\
                \textit{DoS Hulk} F1    & 0.96  & 0.95  \\
                \textit{PortScan} F1    & 0.93  & 0.92  \\
                \bottomrule
            \end{tabular}
            \caption{Performance degradation on perturbed test set.}
        \end{table}

        Although overall accuracy remained relatively high, the ability to recognize \textit{Brute Force} attacks \textbf{collapsed almost completely} (recall $\approx$ 5\%).
        All other classes were virtually unaffected, confirming that the model had learned a \textbf{spurious correlation} between \textbf{Destination Port = 80} and the \textit{Brute Force} label.

        \textbf{Interpretation:}

        This outcome demonstrates a clear \textbf{inductive bias}: the model relies heavily on the \textit{port} feature to detect \textit{Brute Force} traffic, rather than on intrinsic behavioral statistics (e.g., flow duration, packet size).
        Such dependence renders the model non-generalizable, as attacks on other ports would be missed in real-world deployments.

    \subsection{Results —- Experiment 2: Feature Removal}

        After removing the \textit{Destination Port} attribute and re-running preprocessing:

        \begin{table}
            \centering
            \begin{tabular}{lcc}
                \toprule
                \textbf{Stage} & \textbf{Total Samples} & \textbf{PortScan Samples} \\
                \midrule
                Original (raw)                          & 31 507 & 5 000 \\
                After NaN + duplicate removal (no port) & 22 469 & 285   \\
                \bottomrule
            \end{tabular}
            \caption{Class support before and after removing Destination Port.}
        \end{table}

        The \textit{PortScan} class suffered an extreme reduction ($\approx$ -94\%) because many samples previously differed \textbf{only by their destination port value}.
        Once that column was dropped, these entries became \textbf{exact duplicates} and were removed.
        Further inspection confirmed that $\approx$ 4 921 \textit{PortScan} records shared identical flow features except for port numbers, evidencing \textbf{high redundancy} in the dataset.

        A new class-distribution plot (Figure 6) confirmed that the dataset remained \textbf{highly imbalanced}, still dominated by \textit{Benign} traffic.
        Other attack classes (e.g., \textit{Brute Force} and \textit{DoS Hulk}) were also under-represented, though less dramatically.

        % TODO: Insert Figure 6 here showing class distributions after port removal.

    \subsection{Discussion}

        This task provides strong empirical evidence that \textbf{Destination Port} introduces dataset-specific biases rather than genuine discriminative power:
        
        \begin{enumerate}
            \item \textbf{Bias Confirmation}: Changing port 80 to 8080 for \textit{Brute Force} traffic destroyed the classifier's ability to detect that attack type, proving the model had memorized the port-label relationship.
            \item \textbf{Data Redundancy}: Thousands of \textit{PortScan} samples were identical except for the port field, inflating class counts and skewing training.
            \item \textbf{Realism}: In practice, \textit{Brute Force} and \textit{PortScan} attacks can target any port; thus, the observed patterns stem from dataset collection artifacts.
            \item \textbf{Model Reliability}: Including such a biased feature risks over-fitting to protocol- or environment-specific characteristics, undermining cross-network generalization.
        \end{enumerate}

        Consequently, removing or down-weighting the \textit{Destination Port} attribute is justified for the subsequent experiments (Task 4 on loss weighting and Task 5 on deeper networks).
        Its exclusion produces a more reliable though smaller dataset, forcing the model to learn from intrinsic traffic behaviors rather than arbitrary identifiers.
