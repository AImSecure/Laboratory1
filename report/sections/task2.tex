% task2.tex

% Section Title
\section{TASK 2 --- SHALLOW FEED-FORWARD NEURAL NETWORK (FFNN)} \label{sec:shallow-nn}

    \subsection{Model Configuration}

        A \textbf{single-layer Feed-Forward Neural Network (FFNN)} was trained with 32, 64, and 128 neurons to study the effect of network size on learning and generalization. 
        Each model used the Adam optimizer (lr = 0.001), linear activation function, and early stopping over 100 epochs, minimizing categorical cross-entropy on the same partitions defined in Task 2.
        Inputs are standardized features, while outputs are logits over the four classes. 

    \subsection{Training Dynamics}
    
        The training and validation loss curves (Figures~\ref{fig:loss_32}--\ref{fig:loss_128}) show consistent convergence for all models, with rapid loss reduction during early epochs followed by stable plateaus. 
        No overfitting was observed. 
        The 64 and 128-neuron networks achieved the lowest validation loss (\textasciitilde 0.29), both early stopping just before 100 epochs, while the 32-neuron model plateaued slightly higher (0.31).
        
        \vspace{-0.2cm}
        
        \begin{figure}[h]
            \centering
            \begin{subfigure}[b]{0.33\linewidth}
                \centering
                \includegraphics[width=\linewidth]{images/task2/loss_curves_model_32.png}
                \caption{32 neurons}
                \label{fig:loss_32}
            \end{subfigure}
            \begin{subfigure}[b]{0.33\linewidth}
                \centering
                \includegraphics[width=\linewidth]{images/task2/loss_curves_model_64.png}
                \caption{64 neurons}
                \label{fig:loss_64}
            \end{subfigure}
            \begin{subfigure}[b]{0.33\linewidth}
                \centering
                \includegraphics[width=\linewidth]{images/task2/loss_curves_model_128.png}
                \caption{128 neurons}
                \label{fig:loss_128}
            \end{subfigure}
            \caption{Training and validation losses for single-layer FFNNs.}
            \label{fig:loss_comparison}
        \end{figure}

        \vspace{-0.5cm}

    \subsection{Validation Performance Analysis}

        \begin{table}[h]
            \centering
            \begin{minipage}{0.49\textwidth}
                \centering
                \begin{tabular}{lccc}
                    \toprule
                    \textbf{Model} & \textbf{Accuracy} & \textbf{F1 (macro avg)} \\
                    \midrule
                    32 Neurons  & 0.8867 & 0.6769 \\
                    64 Neurons  & 0.8943 & 0.7484 \\
                    128 Neurons & 0.8819 & 0.6717 \\
                    \bottomrule
                \end{tabular}
                \vspace{0.3cm}
                \caption{Validation metrics for linear models.}
                \label{tab:linear-validation}
            \end{minipage}
            \hfill
            \begin{minipage}{0.49\textwidth}
                \centering
                \begin{tabular}{lcccc}
                    \toprule
                    \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
                    \midrule
                    \textbf{0} \textit{Benign}      & 0.8987 & 0.9522 & 0.9247 \\
                    \textbf{1} \textit{Brute Force} & 0.7353 & 0.1748 & 0.2825 \\
                    \textbf{2} \textit{DoS Hulk}    & 0.9869 & 0.8771 & 0.9288 \\
                    \textbf{3} \textit{Port Scan}   & 0.8268 & 0.8907 & 0.8576 \\
                    \bottomrule
                \end{tabular}
                \vspace{0.3cm}
                \caption{Full validation report for the 64-neuron model.}
                \label{tab:linear-metrics}
            \end{minipage}
        \end{table}   

        Validation accuracy reached about 88\% for 32 and 64 neurons and 89.7\% for 128 neurons (macro F1 \textasciitilde 0.76). 
        While the 64-neuron model minimized validation loss, the 128-neuron model yielded the most balanced class performance, correctly identifying minority attacks such as \textit{Brute Force}. 
        Increased neuron count improved representational capacity and class separation.

    \subsection{Activation Function Study and Generalization}
    
        Replacing the linear activation function with ReLU (64 neurons) accelerated convergence and enhanced minority-class recognition (Brute Force F1 = 0.85), as shown in Figure~\ref{fig:relu_64}. 
        Validation and test metrics remained closely aligned, confirming strong generalization. 
        The 64-neuron ReLU network was selected as the reference configuration for subsequent tasks.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.5\linewidth]{images/task2/loss_curves_model_relu_64.png}
            \caption{Loss curves of the 64-neuron model using ReLU activation.}
            \label{fig:relu_64}
        \end{figure}
