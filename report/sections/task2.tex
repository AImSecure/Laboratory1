% task2.tex

% Section Title
\section{TASK 2 --- SHALLOW FEED-FORWARD NEURAL NETWORK (FFNN)} \label{sec:shallow-nn}

    \subsection{Model Configuration}

        A \textbf{single-layer Feed-Forward Neural Network (FFNN)} was trained with 32, 64, and 128 neurons to study the effect of network size on learning and generalization. 
        Each model used the Adam optimizer (lr = 0.0005), linear activation function, and early stopping over 100 epochs, minimizing categorical cross-entropy on the same partitions defined in Task 2.
        Inputs are standardized features, while outputs are logits over the four classes. 

    \subsection{Training Dynamics}
    
        The training and validation loss curves (Figures~\ref{fig:loss_32}--\ref{fig:loss_128}) show consistent convergence for all models, with rapid loss reduction during early epochs followed by stable plateaus. 
        No overfitting was observed. 
        All models converged to similar validation loss levels (\textasciitilde 0.295), with the 64-neuron model early stopping just before 100 epochs.
        
        \vspace{-0.2cm}
        
        \begin{figure}[h]
            \centering
            \begin{subfigure}[b]{0.33\linewidth}
                \centering
                \includegraphics[width=\linewidth]{images/task2/loss_curves_model_32.png}
                \caption{32 neurons}
                \label{fig:loss_32}
            \end{subfigure}
            \begin{subfigure}[b]{0.33\linewidth}
                \centering
                \includegraphics[width=\linewidth]{images/task2/loss_curves_model_64.png}
                \caption{64 neurons}
                \label{fig:loss_64}
            \end{subfigure}
            \begin{subfigure}[b]{0.33\linewidth}
                \centering
                \includegraphics[width=\linewidth]{images/task2/loss_curves_model_128.png}
                \caption{128 neurons}
                \label{fig:loss_128}
            \end{subfigure}
            \caption{Training and validation losses for single-layer FFNNs.}
            \label{fig:loss_comparison}
        \end{figure}

        \vspace{-0.5cm}

    \subsection{Validation Performance Analysis}

        \begin{table}[h]
            \centering
            \begin{minipage}{0.49\textwidth}
                \centering
                \begin{tabular}{lccc}
                    \toprule
                    \textbf{Model} & \textbf{Accuracy} & \textbf{F1 (macro avg)} \\
                    \midrule
                    32 Neurons  & 0.8867 & 0.6769 \\
                    64 Neurons  & 0.8943 & 0.7484 \\
                    128 Neurons & 0.8819 & 0.6717 \\
                    \bottomrule
                \end{tabular}
                \vspace{0.3cm}
                \caption{Validation metrics for linear models.}
                \label{tab:linear-validation}
            \end{minipage}
            \hfill
            \begin{minipage}{0.49\textwidth}
                \centering
                \begin{tabular}{lcccc}
                    \toprule
                    \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
                    \midrule
                    \textbf{0} \textit{Benign}      & 0.8987 & 0.9522 & 0.9247 \\
                    \textbf{1} \textit{Brute Force} & 0.7353 & 0.1748 & 0.2825 \\
                    \textbf{2} \textit{DoS Hulk}    & 0.9869 & 0.8771 & 0.9288 \\
                    \textbf{3} \textit{Port Scan}   & 0.8268 & 0.8907 & 0.8576 \\
                    \bottomrule
                \end{tabular}
                \vspace{0.3cm}
                \caption{Full validation report for the 64-neuron model.}
                \label{tab:linear-metrics}
            \end{minipage}
        \end{table}   

        \noindent Besides loss trajectories, validation metrics (Table~\ref{tab:linear-validation}) highlight performance differences among the three configurations.
        The \textbf{32} and \textbf{128} neuron models achieved good overall accuracy (\textasciitilde 0.88) but failed on the minority \textit{Brute Force} class (precision and recall = 0), primarily learning majority classes (\textit{Benign} and \textit{Port Scan}). 
        The \textbf{64} neuron model delivered the best results (accuracy \textasciitilde 0.894, macro F1 \textasciitilde 0.748), correctly detecting all classes with balanced precision and recall. 
        Considering both the loss trajectories and the validation metrics for this run, the \textbf{64}-neuron model was selected for detailed class-wise analysis.

    \subsection{Activation Function Study and Generalization}
    
        Replacing the linear activation with ReLU (64 neurons) accelerated convergence and markedly improved minority-class recognition.  
        In particular, the Brute Force class (1) F1 rose from 0.28 (linear model; Table~\ref{tab:linear-metrics}) to 0.85 with ReLU (Table~\ref{tab:relu_64-metrics}), showing that ReLU helped capture more complex patterns for this rare attack type.  
        Figure~\ref{fig:relu_64} illustrates the faster/stabler loss dynamics, and validation/test metrics remained closely aligned, confirming good generalization.  
        Overall the model performs best on Benign, DoS Hulk and Port Scan (F1 = 0.96, 0.95, 0.92 respectively) while now also handling Brute Force effectively, indicating a strong across-the-board improvement.

        \begin{figure}[H]
            \centering
            \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\linewidth]{images/task2/loss_curves_model_relu_64.png}
                \caption{Loss curves using ReLU activation.}
                \label{fig:relu_64}
            \end{minipage}
            \hfill
            \begin{minipage}{0.48\textwidth}
                \centering
                \begin{tabular}{lcccc}
                    \toprule
                    \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
                    \midrule
                    \textbf{0} \textit{Benign}      & 0.9622 & 0.9660 & 0.9641 \\
                    \textbf{1} \textit{Brute Force} & 0.7768 & 0.9371 & 0.8494 \\
                    \textbf{2} \textit{DoS Hulk}    & 0.9972 & 0.9172 & 0.9555 \\
                    \textbf{3} \textit{Port Scan}   & 0.9332 & 0.9216 & 0.9274 \\
                    \midrule
                \end{tabular}
                \begin{tabular}{c c c}
                    \textbf{Accuracy} &  \textbf{Macro F1} & \textbf{Weighted F1} \\
                    0.9508 & 0.9241 & 0.9513 \\
                \end{tabular}
                \vspace{0.3cm}
                \caption{Validation set report.}
                \label{tab:relu_64-metrics}
            \end{minipage}
        \end{figure}
