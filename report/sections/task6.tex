% task6.tex

% Section Title
\section{TASK 6 --- OVERFITTING AND REGULARIZATION TECHNIQUES} \label{sec:overfitting-regularization}

    In this task, various regularization techniques were explored to mitigate overfitting, starting from a new baseline deep model with 6 layers ([256,128,64,32,16,8]), ReLU activations and the AdamW optimizer.

    \subsection{Baseline Model}
    
        The baseline deep model (\textit{AdamW}, no explicit regularization) shows consistent convergence with both training and validation losses stabilizing (final Train Loss $\approx$ 0.1022, Val Loss $\approx$ 0.1188; see Figure~\ref{fig:baseline_loss}).
        The validation loss remains slightly higher than the training loss, indicating good generalization rather than strong overfitting. Both curves plateau together. The final validation and test accuracies are high (Validation accuracy = 96.24\%, Test accuracy = 96.46\%).

        \begin{figure}[h]
            \centering
            \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\linewidth]{images/task6/Baseline_loss_curve.png}
                \caption{Loss curves for the baseline model.}
                \label{fig:baseline_loss}
            \end{minipage}
            \hfill
            \begin{minipage}{0.48\textwidth}
                \centering
                \begin{tabular}{lcccc}
                    \toprule
                    \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
                    \midrule
                    \textbf{0} \textit{Benign}      & 0.9667 & 0.9876 & 0.9770 \\
                    \textbf{1} \textit{Brute Force} & 0.9249 & 0.9476 & 0.9361 \\
                    \textbf{2} \textit{DoS Hulk}    & 0.9817 & 0.9017 & 0.9400 \\
                    \textbf{3} \textit{Port Scan}   & 0.7750 & 0.5439 & 0.6392 \\
                    \midrule
                \end{tabular}
                \begin{tabular}{c c c}
                    \textbf{Accuracy} &  \textbf{Macro F1} & \textbf{Weighted F1} \\
                    0.9646 & 0.8731 & 0.9638 \\
                \end{tabular}
                \vspace{0.3cm}
                \captionof{table}{Test set report.}
                \label{tab:lr-metrics}
            \end{minipage}
        \end{figure}

    \subsection{Effect of Normalization and Regularization}

        Several regularization strategies were applied to investigate their impact on convergence and performance:
        \begin{itemize}
            \item \textbf{Dropout (0.5)}: Increased generalization pressure but produced weaker overall accuracy (Validation accuracy = 94.39\%, Test accuracy = 93.93\%). Final losses (Train $\approx$ 0.1462, Val $\approx$ 0.1339) show the validation loss can be lower than the training loss; the minority class (class 3) was not predicted (zero precision/recall / F1 = 0.0).
            \item \textbf{Batch Normalization}: Produced less stable validation behaviour (final Train $\approx$ 0.1269, Val $\approx$ 0.2123) with a higher validation loss at the end of training, suggesting sensitivity to batch statistics on this tabular dataset. Validation/Test accuracies remained relatively high (Val = 95.51\%, Test = 95.73\%) but per-class recall for the minority class was degraded.
            \item \textbf{BatchNorm + Dropout (0.5)}: The combination appears to over-regularize the model: training and validation losses are higher than the baseline (final Train $\approx$ 0.1922, Val $\approx$ 0.1737) and overall accuracy falls to $\approx$ 93\%. Moreover, the model made no predictions for the minority class (F1 = 0.0), which suggests BatchNorm+Dropout harmed the decision boundary for rare classes (likely due to noisy batch statistics and excessive noise injection).
            \item \textbf{Weight Decay (1e-4)}: L2 regularization via weight decay yielded stable behaviour (final Train $\approx$ 0.1073, Val $\approx$ 0.1305) and a good balance between stability and minority-class performance. Validation/Test accuracies remained close to the baseline (Val = 95.68\%, Test = 95.86\%) and recall for the minority class improved compared to all the other configurations (Baseline excluded).
            \item \textbf{Weight Decay + BN + Dropout (0.5)}: Combination of strong regularizers led to underfitting (final Train $\approx$ 0.1987, Val $\approx$ 0.1708) and reduced accuracy (Val = 93.99\%, Test = 93.55\%); the minority class was again not detected in most runs.
        \end{itemize}

        \begin{figure}[h]
            \centering
            \begin{minipage}{0.48\textwidth}
                \centering
                \includegraphics[width=\linewidth]{images/task6/WeightDecay_1e-4_loss_curve.png}
                \caption{Loss curves using Weight Decay (1e-4).}
                \label{fig:baseline_loss}
            \end{minipage}
            \hfill
            \begin{minipage}{0.48\textwidth}
                \centering
                \begin{tabular}{lcccc}
                    \toprule
                    \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
                    \midrule
                    \textbf{0} \textit{Benign}      & 0.9695 & 0.9787 & 0.9741 \\
                    \textbf{1} \textit{Brute Force} & 0.9340 & 0.9439 & 0.9389 \\
                    \textbf{2} \textit{DoS Hulk}    & 0.9588 & 0.9031 & 0.9301 \\
                    \textbf{3} \textit{Port Scan}   & 0.3881 & 0.4561 & 0.4194 \\
                    \midrule
                \end{tabular}
                \begin{tabular}{c c c}
                    \textbf{Accuracy} &  \textbf{Macro F1} & \textbf{Weighted F1} \\
                    0.9568 & 0.8156 & 0.9572 \\
                \end{tabular}
                \vspace{0.3cm}
                \captionof{table}{Validation set report.}
                \label{tab:lr-metrics}
            \end{minipage}
        \end{figure}

    \subsection{Final Observations}

        The experiments show a clear trade-off between overall performance (accuracy / weighted metrics) and robustness on the rare class.

        The baseline produced the highest overall scores in these runs (Validation = 96.24\%, Test = 96.46\%) and also the best minority-class F1 among the configurations tested. 
        \textbf{AdamW + light weight decay ($1\times10^{-4}$)} yielded slightly lower overall accuracy but improved stability and in some cases the minority-class recall compared to aggressive normalization schemes.
        Stronger regularizers (Dropout 0.5, BatchNorm combined with Dropout) tended to underfit this tabular task: they reduced overall accuracy and frequently led to the minority class being ignored (zero precision/recall in several runs).

        \noindent
        If the project's primary objective is overall accuracy and weighted F1, the \textbf{baseline} is the preferred choice. 
        However, if improving minority-class detection is critical, a mild regularization strategy like \textbf{Weight Decay (1e-4)} may be beneficial, despite a small drop in overall accuracy.
