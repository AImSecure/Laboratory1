{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaRjftvSROvg"
      },
      "source": [
        "# Laboratory 1 — Feed Forward Neural Networks (AI & Cybersecurity)\n",
        "\n",
        "This notebook follows the lab brief in `lab/Lab1_FFNN.txt` and is organized into tasks:\n",
        "- Task 1: Data preprocessing (cleaning, splitting, outliers, normalization)\n",
        "- Task 2: Shallow NN (1 layer), train sizes {32, 64, 128}, metrics and analysis; then ReLU change\n",
        "- Task 3: Impact of specific features (Destination Port), bias test and port removal\n",
        "- Task 4: Loss function impact (weighted CrossEntropy)\n",
        "- Task 5: Deep NN, batch size, optimizer comparisons\n",
        "- Task 6: Overfitting and regularization (dropout, batchnorm, weight decay)\n",
        "\n",
        "Deliverables: executed notebook with visible results and an HTML export; code should be runnable on Linux locally (no Google Drive mount)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svE8FspeRPW3"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZXgjER_TsGp"
      },
      "outputs": [],
      "source": [
        "# --- Install required libraries ---\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install numpy pandas scikit-learn matplotlib seaborn\n",
        "!pip install tqdm\n",
        "\n",
        "# --- Import libraries ---\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rFNp6fgTv9w"
      },
      "source": [
        "## Paths and project setup\n",
        "We run locally. The dataset is expected at `lab/data/dataset_lab_1.csv`. We'll create folders for results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAnAndzpTxe5"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqMS_38yTvFN"
      },
      "outputs": [],
      "source": [
        "base_path = 'content/drive/MyDrive/'\n",
        "project_path = base_path + 'Projects/AImSecure/Laboratory1/'\n",
        "data_path = project_path + 'data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dswv2Je3UA5Y"
      },
      "source": [
        "## Task 1 — Data preprocessing\n",
        "What we will do:\n",
        "- Load CSV from `lab/data/dataset_lab_1.csv`\n",
        "- Inspect basic info, class distribution, and feature ranges\n",
        "- Remove NaN and duplicate rows; report counts before/after\n",
        "- Split into train/val/test with stratify (60/20/20, fixed SEED)\n",
        "- Inspect outliers on train/val (e.g., boxplots, z-scores) and choose normalization\n",
        "- Fit scaler on train only; transform val/test; persist scaler if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vEJrc85UCuJ"
      },
      "outputs": [],
      "source": [
        "# Load and initial inspection\n",
        "assert DATA_PATH.exists(), f\"Dataset not found at {DATA_PATH}\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Shape (raw):\", df.shape)\n",
        "print(\"Columns:\", list(df.columns))\n",
        "print(df.head(3))\n",
        "\n",
        "# Basic info\n",
        "print(\"\\nLabel distribution (raw):\")\n",
        "print(df['Label'].value_counts(dropna=False))\n",
        "\n",
        "# Remove NaN and duplicates\n",
        "raw_n = len(df)\n",
        "df = df.dropna()\n",
        "df = df.drop_duplicates()\n",
        "clean_n = len(df)\n",
        "print(f\"Removed {raw_n-clean_n} rows (NaN+dupes). New shape: {df.shape}\")\n",
        "\n",
        "# Split features/target\n",
        "label_col = 'Label'\n",
        "feature_cols = [c for c in df.columns if c != label_col]\n",
        "X = df[feature_cols]\n",
        "y = df[label_col]\n",
        "\n",
        "# Train/val/test split 60/20/20 with stratify\n",
        "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
        "    X, y, test_size=0.4, stratify=y, random_state=SEED\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_tmp, y_tmp, test_size=0.5, stratify=y_tmp, random_state=SEED\n",
        ")\n",
        "print(\"Splits:\", X_train.shape, X_val.shape, X_test.shape)\n",
        "print(\"Train label counts:\\n\", y_train.value_counts())\n",
        "print(\"Val label counts:\\n\", y_val.value_counts())\n",
        "print(\"Test label counts:\\n\", y_test.value_counts())\n",
        "\n",
        "# Outlier quick-look: z-score counts per feature on train\n",
        "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "zs = ((X_train[num_cols] - X_train[num_cols].mean())/X_train[num_cols].std(ddof=0)).abs()\n",
        "outlier_counts = (zs > 3).sum().sort_values(ascending=False)\n",
        "print(\"\\nOutliers (>3 std) per feature on train:\\n\", outlier_counts.head(10))\n",
        "\n",
        "# Normalization: StandardScaler on train only\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train[num_cols])\n",
        "X_val_s = scaler.transform(X_val[num_cols])\n",
        "X_test_s = scaler.transform(X_test[num_cols])\n",
        "\n",
        "# Persist minimal metadata\n",
        "scaler_meta = {\n",
        "    \"type\":\"StandardScaler\",\n",
        "    \"fit_on\":\"train\",\n",
        "    \"num_features\": num_cols,\n",
        "}\n",
        "with open(METRICS_DIR/\"scaler_meta.json\", \"w\") as f:\n",
        "    json.dump(scaler_meta, f, indent=2)\n",
        "\n",
        "print(\"Normalization complete. Shapes:\", X_train_s.shape, X_val_s.shape, X_test_s.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QJSBi_iRPxw"
      },
      "source": [
        "## PyTorch datasets and loaders\n",
        "We convert standardized NumPy arrays to tensors and build dataloaders with a configurable batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label encoding\n",
        "classes = sorted(pd.unique(y))\n",
        "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
        "y_train_i = y_train.map(class_to_idx).to_numpy()\n",
        "y_val_i = y_val.map(class_to_idx).to_numpy()\n",
        "y_test_i = y_test.map(class_to_idx).to_numpy()\n",
        "\n",
        "# Tensors\n",
        "X_train_t = torch.tensor(X_train_s, dtype=torch.float32)\n",
        "X_val_t   = torch.tensor(X_val_s,   dtype=torch.float32)\n",
        "X_test_t  = torch.tensor(X_test_s,  dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train_i, dtype=torch.long)\n",
        "y_val_t   = torch.tensor(y_val_i,   dtype=torch.long)\n",
        "y_test_t  = torch.tensor(y_test_i,  dtype=torch.long)\n",
        "\n",
        "INPUT_DIM = X_train_t.shape[1]\n",
        "NUM_CLASSES = len(classes)\n",
        "print(\"Input dim:\", INPUT_DIM, \"Classes:\", NUM_CLASSES, classes)\n",
        "\n",
        "# Dataloaders\n",
        "BATCH_SIZE = 64\n",
        "train_ds = TensorDataset(X_train_t, y_train_t)\n",
        "val_ds   = TensorDataset(X_val_t, y_val_t)\n",
        "test_ds  = TensorDataset(X_test_t, y_test_t)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training utilities\n",
        "Helper functions: training loop with early stopping, evaluation, and plotting loss curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion=None):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_preds, all_targets = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        if criterion is not None:\n",
        "            total_loss += criterion(logits, yb).item() * xb.size(0)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_targets.append(yb.cpu().numpy())\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "    avg_loss = (total_loss / len(loader.dataset)) if criterion is not None else None\n",
        "    return avg_loss, all_preds, all_targets\n",
        "\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=10, mode='min'):\n",
        "        self.patience = patience\n",
        "        self.mode = mode\n",
        "        self.best = None\n",
        "        self.count = 0\n",
        "    def step(self, metric):\n",
        "        if self.best is None:\n",
        "            self.best = metric\n",
        "            return False\n",
        "        improve = metric < self.best if self.mode == 'min' else metric > self.best\n",
        "        if improve:\n",
        "            self.best = metric\n",
        "            self.count = 0\n",
        "            return False\n",
        "        else:\n",
        "            self.count += 1\n",
        "            return self.count >= self.patience\n",
        "\n",
        "def plot_losses(train_losses, val_losses, title, save_path=None):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(train_losses, label='train')\n",
        "    plt.plot(val_losses, label='val')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2 — Shallow Neural Network (1 layer)\n",
        "We will:\n",
        "- Define a single-hidden-layer FFNN with hidden sizes h in {32, 64, 128}\n",
        "- Use Linear activation as requested (i.e., no nonlinearity) for the first run\n",
        "- Train with AdamW, lr=5e-4, batch=64, CE loss, up to 100 epochs with early stopping\n",
        "- Plot train/val losses, select best by lowest val loss, report validation classification\n",
        "- Evaluate best on test and compare to validation\n",
        "- Then change activation to ReLU for the best width and retrain; discuss change"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ReLU variant for the best width\n",
        "Train the same architecture but with ReLU activation, then compare validation report to the linear version. Do NOT test-compare unless justified (overfitting risk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train ReLU variant for the best width\n",
        "best_h = best['h']\n",
        "relu_tag = f\"shallow_relu_h{best_h}\"\n",
        "model_relu = ShallowFFNN(INPUT_DIM, best_h, NUM_CLASSES, activation='relu')\n",
        "model_relu, tr_r, va_r = train_model(model_relu, train_loader, val_loader, epochs=100, lr=5e-4, patience=10, tag=relu_tag)\n",
        "va_loss_r, va_preds_r, va_tgts_r = evaluate(model_relu, val_loader, nn.CrossEntropyLoss())\n",
        "print(\"Validation report (ReLU, best width)\")\n",
        "print(classification_report(va_tgts_r, va_preds_r, target_names=classes))\n",
        "print(\"Note: Comparing on test at this stage may be misleading; validate design choices first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3 — Impact of specific features (Destination Port)\n",
        "Steps:\n",
        "1) Modify only the test split: for rows with Label==Brute Force and Destination Port==80, change to 8080. Re-run inference with the best model and compare test metrics to validation baseline.\n",
        "2) Remove the Destination Port feature entirely from the original dataset; redo preprocessing (cleaning, split, scaling); report PortScan counts before/after duplicates removal; discuss class balance.\n",
        "Note: Keep a copy of the original splits to ensure fair comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1: Modify Destination Port for Brute Force in Test and re-evaluate\n",
        "if 'Destination Port' in feature_cols:\n",
        "    # Build a modifiable DataFrame for test\n",
        "    test_df = X_test.copy()\n",
        "    test_df['Label'] = y_test.values\n",
        "    before_counts = (test_df[(test_df['Label']=='Brute Force') & (test_df['Destination Port']==80)].shape[0])\n",
        "    # Change 80 -> 8080 for Brute Force\n",
        "    mask = (test_df['Label']=='Brute Force') & (test_df['Destination Port']==80)\n",
        "    test_df.loc[mask, 'Destination Port'] = 8080\n",
        "    after_counts = (test_df[(test_df['Label']=='Brute Force') & (test_df['Destination Port']==80)].shape[0])\n",
        "    print(f\"Changed Brute Force port 80 -> 8080 in test: {before_counts} rows affected, now {after_counts} with port==80\")\n",
        "    \n",
        "    # Re-standardize using the SAME scaler (train fit)\n",
        "    test_df_std = scaler.transform(test_df[num_cols])\n",
        "    X_test_mod_t = torch.tensor(test_df_std, dtype=torch.float32)\n",
        "    y_test_mod_t = torch.tensor(y_test_i, dtype=torch.long)\n",
        "    test_mod_loader = DataLoader(TensorDataset(X_test_mod_t, y_test_mod_t), batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Inference with the best validation model (from Task 2)\n",
        "    mod_loss, mod_preds, mod_tgts = evaluate(best_model, test_mod_loader, nn.CrossEntropyLoss())\n",
        "    print(\"Modified test loss:\", round(mod_loss, 4))\n",
        "    print(classification_report(mod_tgts, mod_preds, target_names=classes))\n",
        "else:\n",
        "    print(\"Destination Port feature not found; skip part 3.1.\")\n",
        "\n",
        "# 3.2: Remove Destination Port from the original dataset and redo preprocessing\n",
        "if 'Destination Port' in feature_cols:\n",
        "    noport_df = df.drop(columns=['Destination Port'])\n",
        "    print(\"Shape without Destination Port:\", noport_df.shape)\n",
        "    # Remove duplicates specifically and count PortScan before/after\n",
        "    portscan_before = df[df['Label']=='PortScan'].shape[0]\n",
        "    noport_df_nodup = noport_df.drop_duplicates()\n",
        "    labels_noport = noport_df_nodup['Label']\n",
        "    portscan_after = noport_df_nodup[labels_noport=='PortScan'].shape[0]\n",
        "    print(f\"PortScan count before: {portscan_before}, after dropping duplicates (no port): {portscan_after}\")\n",
        "    print(\"Class balance (no port, after dedupe):\\n\", labels_noport.value_counts())\n",
        "else:\n",
        "    print(\"Destination Port feature not present; already removed earlier.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4 — Impact of Loss Function (class weighting)\n",
        "- Compute class weights from the training partition only, using sklearn `compute_class_weight(class_weight='balanced')`\n",
        "- Retrain the best architecture (from Task 2 or after port removal, depending on stage)\n",
        "- Compare per-class metrics, accuracy, and F1 against unweighted run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute class weights from TRAIN partition\n",
        "classes_arr = np.array(classes)\n",
        "weights = compute_class_weight(class_weight='balanced', classes=classes_arr, y=y_train.values)\n",
        "print(\"Class weights (train):\", dict(zip(classes, np.round(weights,3))))\n",
        "\n",
        "# Map weights to class index order 0..NUM_CLASSES-1\n",
        "weight_vec = np.array([weights[list(classes_arr).index(c)] for c in classes])\n",
        "\n",
        "# Retrain best shallow width with weights (use ReLU variant for stronger baseline)\n",
        "weighted_tag = f\"shallow_weighted_h{best_h}\"\n",
        "weighted_model = ShallowFFNN(INPUT_DIM, best_h, NUM_CLASSES, activation='relu')\n",
        "weighted_model, tr_w, va_w = train_model(\n",
        "    weighted_model, train_loader, val_loader, epochs=100, lr=5e-4, patience=10, tag=weighted_tag,\n",
        "    weight=weight_vec\n",
        ")\n",
        "va_loss_w, va_preds_w, va_tgts_w = evaluate(weighted_model, val_loader, nn.CrossEntropyLoss())\n",
        "print(\"Validation (weighted):\")\n",
        "print(classification_report(va_tgts_w, va_preds_w, target_names=classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 5 — Deep Neural Networks, Batch Size, Optimizers\n",
        "We will:\n",
        "- Try depths L in {3, 4, 5} and for each try 2 hidden-size patterns (total 6)\n",
        "- Use ReLU, AdamW, lr=5e-4, epochs<=50 with early stopping\n",
        "- Select best by validation; evaluate best on test\n",
        "- Compare batch sizes {4, 64, 256, 1024} on best arch: report validation metrics and wall-clock time\n",
        "- Compare optimizers: SGD, SGD+Momentum (0.1, 0.5, 0.9), AdamW; analyze losses and times; then tune LR/epochs for the best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeepFFNN(nn.Module):\n",
        "    def __init__(self, input_dim, layers, num_classes):\n",
        "        super().__init__()\n",
        "        dims = [input_dim] + layers\n",
        "        mods = []\n",
        "        for i in range(len(dims)-1):\n",
        "            mods.append(nn.Linear(dims[i], dims[i+1]))\n",
        "            mods.append(nn.ReLU())\n",
        "        mods.append(nn.Linear(dims[-1], num_classes))\n",
        "        self.net = nn.Sequential(*mods)\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Architectures to try: two per depth 3,4,5\n",
        "arch_grid = {\n",
        "    3: [[32,16,8],[64,32,16]],\n",
        "    4: [[64,32,16,8],[32,32,16,8]],\n",
        "    5: [[128,64,32,16,8],[32,32,32,16,8]],\n",
        "}\n",
        "\n",
        "arch_results = []\n",
        "for depth, patterns in arch_grid.items():\n",
        "    for pattern in patterns:\n",
        "        tag = f\"deep_L{depth}_h{'-'.join(map(str,pattern))}\"\n",
        "        model = DeepFFNN(INPUT_DIM, pattern, NUM_CLASSES)\n",
        "        model, tr_d, va_d = train_model(model, train_loader, val_loader, epochs=50, lr=5e-4, patience=8, tag=tag)\n",
        "        va_loss_d, va_preds_d, va_tgts_d = evaluate(model, val_loader, nn.CrossEntropyLoss())\n",
        "        rep_d = classification_report(va_tgts_d, va_preds_d, target_names=classes, output_dict=True)\n",
        "        arch_results.append({\"depth\":depth, \"pattern\":pattern, \"val_loss\":va_loss_d, \"report\":rep_d, \"model\":model, \"tag\":tag})\n",
        "\n",
        "best_deep = min(arch_results, key=lambda r: r['val_loss'])\n",
        "print(\"Best deep arch:\", best_deep['pattern'], \"(L=\", len(best_deep['pattern']), \") val_loss=\", round(best_deep['val_loss'],4))\n",
        "\n",
        "# Evaluate best deep on test\n",
        "best_deep_model = best_deep['model']\n",
        "loss_td, preds_td, tgts_td = evaluate(best_deep_model, test_loader, nn.CrossEntropyLoss())\n",
        "print(\"Test report (best deep):\")\n",
        "print(classification_report(tgts_td, preds_td, target_names=classes))\n",
        "\n",
        "# Batch size experiment\n",
        "for bs in [4, 64, 256, 1024]:\n",
        "    tl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
        "    vl = DataLoader(val_ds, batch_size=bs)\n",
        "    model = DeepFFNN(INPUT_DIM, best_deep['pattern'], NUM_CLASSES)\n",
        "    start = time.time()\n",
        "    model, tr_b, va_b = train_model(model, tl, vl, epochs=50, lr=5e-4, patience=8, tag=f\"deep_bs{bs}\")\n",
        "    elapsed = time.time()-start\n",
        "    va_loss_b, va_preds_b, va_tgts_b = evaluate(model, vl, nn.CrossEntropyLoss())\n",
        "    print(f\"BS={bs}: val_loss={va_loss_b:.4f}, time={elapsed:.1f}s\")\n",
        "\n",
        "# Optimizer experiment\n",
        "optims = [\n",
        "    (\"SGD\", lambda params: optim.SGD(params, lr=5e-3)),\n",
        "    (\"SGD_m0.1\", lambda params: optim.SGD(params, lr=5e-3, momentum=0.1)),\n",
        "    (\"SGD_m0.5\", lambda params: optim.SGD(params, lr=5e-3, momentum=0.5)),\n",
        "    (\"SGD_m0.9\", lambda params: optim.SGD(params, lr=5e-3, momentum=0.9)),\n",
        "    (\"AdamW\", lambda params: optim.AdamW(params, lr=5e-4)),\n",
        "]\n",
        "\n",
        "opt_results = []\n",
        "for name, factory in optims:\n",
        "    model = DeepFFNN(INPUT_DIM, best_deep['pattern'], NUM_CLASSES).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = factory(model.parameters())\n",
        "    es = EarlyStopper(patience=8, mode='min')\n",
        "    tr_losses, va_losses = [], []\n",
        "    start = time.time()\n",
        "    for ep in range(50):\n",
        "        tr = train_one_epoch(model, train_loader, criterion, optimizer)\n",
        "        va, _, _ = evaluate(model, val_loader, criterion)\n",
        "        tr_losses.append(tr); va_losses.append(va)\n",
        "        if es.step(va):\n",
        "            break\n",
        "    elapsed = time.time()-start\n",
        "    va_loss_o, va_preds_o, va_tgts_o = evaluate(model, val_loader, criterion)\n",
        "    opt_results.append((name, va_loss_o, elapsed))\n",
        "    plot_losses(tr_losses, va_losses, f\"opt_{name}\", save_path=FIG_DIR/f\"opt_{name}_loss.png\")\n",
        "\n",
        "print(\"Optimizer comparison (name, val_loss, time_s):\", [(n, round(l,4), round(t,1)) for n,l,t in opt_results])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 6 — Overfitting and Regularization\n",
        "- Build 6-layer FFNN with widths [256, 128, 64, 32, 16] (the final layer is num_classes)\n",
        "- Train with ReLU, AdamW lr=5e-4, batch=128, epochs=50\n",
        "- Inspect losses for signs of overfitting\n",
        "- Add Dropout and BatchNorm variants; try weight decay (AdamW) and compare validation/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RegFFNN(nn.Module):\n",
        "    def __init__(self, input_dim, widths, num_classes, dropout=0.0, batchnorm=False):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev = input_dim\n",
        "        for w in widths:\n",
        "            layers.append(nn.Linear(prev, w))\n",
        "            if batchnorm:\n",
        "                layers.append(nn.BatchNorm1d(w))\n",
        "            layers.append(nn.ReLU())\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            prev = w\n",
        "        layers.append(nn.Linear(prev, num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Baseline (no regularization)\n",
        "widths6 = [256,128,64,32,16]\n",
        "BATCH_SIZE = 128\n",
        "train_loader6 = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader6   = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "model6 = RegFFNN(INPUT_DIM, widths6, NUM_CLASSES, dropout=0.0, batchnorm=False)\n",
        "model6, tr6, va6 = train_model(model6, train_loader6, val_loader6, epochs=50, lr=5e-4, patience=8, tag=\"reg_L6_base\")\n",
        "va_loss6, va_preds6, va_tgts6 = evaluate(model6, val_loader6, nn.CrossEntropyLoss())\n",
        "print(\"Validation (no regularization):\")\n",
        "print(classification_report(va_tgts6, va_preds6, target_names=classes))\n",
        "\n",
        "# Dropout + BatchNorm + weight decay variants\n",
        "for dp in [0.2, 0.5]:\n",
        "    for bn in [False, True]:\n",
        "        tag = f\"reg_L6_dp{dp}_bn{int(bn)}\"\n",
        "        m = RegFFNN(INPUT_DIM, widths6, NUM_CLASSES, dropout=dp, batchnorm=bn).to(device)\n",
        "        weight_decay = 1e-4 if bn else 5e-4\n",
        "        # Custom train loop to set weight decay\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(m.parameters(), lr=5e-4, weight_decay=weight_decay)\n",
        "        es = EarlyStopper(patience=8, mode='min')\n",
        "        tr_losses, va_losses = [], []\n",
        "        for ep in range(50):\n",
        "            tr = train_one_epoch(m, train_loader6, criterion, optimizer)\n",
        "            va, _, _ = evaluate(m, val_loader6, criterion)\n",
        "            tr_losses.append(tr); va_losses.append(va)\n",
        "            if es.step(va):\n",
        "                break\n",
        "        plot_losses(tr_losses, va_losses, tag, save_path=FIG_DIR/f\"{tag}_loss.png\")\n",
        "        vLoss, vp, vt = evaluate(m, val_loader6, criterion)\n",
        "        print(f\"Variant {tag} — val_loss={vLoss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ShallowFFNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, activation='linear'):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc_out = nn.Linear(hidden_dim, num_classes)\n",
        "        self.activation = activation\n",
        "        if activation == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        else:\n",
        "            self.act = nn.Identity()\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.fc1(x))\n",
        "        return self.fc_out(x)\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=100, lr=5e-4, weight=None, patience=10, tag=\"model\"):\n",
        "    model = model.to(device)\n",
        "    weight_t = torch.tensor(weight, dtype=torch.float32, device=device) if weight is not None else None\n",
        "    criterion = nn.CrossEntropyLoss(weight=weight_t)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    es = EarlyStopper(patience=patience, mode='min')\n",
        "\n",
        "    tr_losses, va_losses = [] , []\n",
        "    best_state, best_va = None, float('inf')\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        tr = train_one_epoch(model, train_loader, criterion, optimizer)\n",
        "        va, _, _ = evaluate(model, val_loader, criterion)\n",
        "        tr_losses.append(tr)\n",
        "        va_losses.append(va)\n",
        "        if va < best_va:\n",
        "            best_va = va\n",
        "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "        if es.step(va):\n",
        "            break\n",
        "    # restore best\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    # plot\n",
        "    plot_losses(tr_losses, va_losses, f\"{tag} losses\", save_path=FIG_DIR/f\"{tag}_loss.png\")\n",
        "    return model, tr_losses, va_losses\n",
        "\n",
        "# Train three widths with linear activation\n",
        "hidden_sizes = [32, 64, 128]\n",
        "results = []\n",
        "for h in hidden_sizes:\n",
        "    tag = f\"shallow_linear_h{h}\"\n",
        "    m = ShallowFFNN(INPUT_DIM, h, NUM_CLASSES, activation='linear')\n",
        "    m, tr, va = train_model(m, train_loader, val_loader, epochs=100, lr=5e-4, patience=10, tag=tag)\n",
        "    va_loss, va_preds, va_tgts = evaluate(m, val_loader, nn.CrossEntropyLoss())\n",
        "    rep = classification_report(va_tgts, va_preds, target_names=classes, output_dict=True)\n",
        "    results.append({\"h\":h, \"val_loss\":va_loss, \"report\":rep, \"model\":m})\n",
        "    # save report\n",
        "    with open(METRICS_DIR/f\"{tag}_val_report.json\", \"w\") as f:\n",
        "        json.dump(rep, f, indent=2)\n",
        "\n",
        "# Select best by lowest val loss\n",
        "best = min(results, key=lambda r: r[\"val_loss\"]) \n",
        "print(\"Best width (linear):\", best[\"h\"], \"val_loss:\", best[\"val_loss\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validation reports for the 3 linear models (summary)\n",
        "for r in results:\n",
        "    h = r['h']\n",
        "    rep = r['report']\n",
        "    print(f\"\\nWidth {h} — Val loss: {r['val_loss']:.4f}\")\n",
        "    # Compact summary per class + macro/weighted\n",
        "    keys = list(classes) + ['macro avg','weighted avg','accuracy']\n",
        "    compact = {}\n",
        "    for k in keys:\n",
        "        if k in rep:\n",
        "            if k == 'accuracy':\n",
        "                compact[k] = round(rep[k], 4)\n",
        "            else:\n",
        "                compact[k] = {m: round(rep[k][m], 4) for m in ['precision','recall','f1-score'] if m in rep[k]}\n",
        "    print(compact)\n",
        "\n",
        "# Test evaluation for best linear model\n",
        "best_model = best['model']\n",
        "test_loss, test_preds, test_tgts = evaluate(best_model, test_loader, nn.CrossEntropyLoss())\n",
        "print(\"\\nTest loss (best linear):\", round(test_loss, 4))\n",
        "print(\"Validation vs Test — compare metrics (test shown below)\")\n",
        "print(classification_report(test_tgts, test_preds, target_names=classes))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
